{
  
    
        "post0": {
            "title": "Personal Project",
            "content": "Forecasting retail turnover of household goods in the Australian economy . Ian Dias . Nov 2020 . Contents . 1. Visualization of the Time-Series . 2. Stationarity Checks . 3. Predictors . 3.1 Naive Predictor . 3.2 AR Predictor . 3.3 Linear Regression - Seasonality Controls . 3.4 Holt Winters Smoothing . 3.5 Adding external variables . 3.6 Vector AutoRegression . 4. Summary . 5. Predictions . Introduction . I plan to forecast the monthly retail turnover of household goods in the Australian wide economy. The ABS has collected this data from May of 1982 and the last data available is until August 2020. The amount of household goods being traded is an important indication of an economy - as the goods range from electrical, to househod building and furniture. It appears that the more individuals are able to spend on these goods the seemingly more satisfaction and utility they achieve. . Due to the recent impacts of the COVID-19 pandemic, it would be of interest how this has affected the household goods industry. The Australian economy as a whole has reduced, but given that consumers are spending more time at home the impact it has had on this specific industry might not be completely obvious. . The data is accessed from: https://www.abs.gov.au/statistics/industry/retail-and-wholesale-trade/retail-trade-australia/latest-release . 1. Visualization of the time-series . import pandas as pd from matplotlib import pyplot as plt import seaborn as sns import numpy as np from statsmodels.graphics.tsaplots import plot_acf from statsmodels.graphics.tsaplots import plot_pacf sns.set_style(&#39;darkgrid&#39;) plt.rcParams[&quot;figure.figsize&quot;] = (10,5) . def reset(data=&#39;850101.xls&#39;): df = pd.read_excel(data,usecols=[0,2],sheet_name = &#39;Data1&#39;,skiprows=&#39;5&#39;) df = df[10:] df.set_index(&#39;Unnamed: 0&#39;, inplace=True) df.index.name = &#39;Dates&#39; df.columns = [&#39;Household goods&#39;] df.index = pd.to_datetime(df.index) return df df = reset() . def plot(x,title=None,baseline=None, **kwargs): plt.plot(x, **kwargs) if baseline is not None: plt.plot(baseline, label=&#39;Baseline&#39;, alpha=0.5) plt.legend() plt.title(title) . plot(df, title=&#39;Retail Household Goods&#39;) . There seems to be both clear seasonality and and upwards trend to the data. The data will be isolated by year to identify the seasonality patterns and the impacts of the trend. . Visualizing the data and patterns . df[&#39;year&#39;] = df.index.year df[&#39;month&#39;] = df.index.month years = df[&#39;year&#39;].unique() for i, y in enumerate(years): plt.plot(&#39;month&#39;, &#39;Household goods&#39;, data=df[df[&#39;year&#39;] == y], label=y) if i % 4 == 0 or y == 2020: plt.text(df[df[&#39;year&#39;] == y].shape[0], df[df[&#39;year&#39;] == y][&#39;Household goods&#39;][-1:].values[0], y) . Looking at the above graph, it appears that there is a definite upward tick for December and there could possibly be monthly or quarterly variation in the data.The mean also appears to be higher every year. There seems to be a significant increase in spending in 2020 starting from February and going on until the last value of October. This is most likely due to the COVID-19 virus that is impacting the world and began its Australian penetration in March. As more individuals are under lockdown they are staying home longer and this may increase the quantity of household goods they would typically require. . 2. Stationarity checks . df = reset() . from statsmodels.tsa.stattools import adfuller first_diff = df.diff() def dickey_fuller(x): &#39;&#39;&#39;Dickey-fuller test to check for stationarity&#39;&#39;&#39; result = adfuller(x) print(&#39;ADF Test statistic: %f&#39; % result[0]) print(&#39;p-value: %f&#39; % result[1]) print(&#39;The critical values are: {}&#39; .format(result[4])) for key,value in result[4].items(): if value &gt; result[0]: print(&#39;There is a unit root present and the null hypothesis is rejected. The series is stationary.&#39;) return print(&#39;The null hypothesis is failed to be rejected. The series is not stationary.&#39;) dickey_fuller(df) . ADF Test statistic: 1.572934 p-value: 0.997774 The critical values are: {&#39;1%&#39;: -3.4451307246616514, &#39;5%&#39;: -2.86805689169311, &#39;10%&#39;: -2.570241263729327} The null hypothesis is failed to be rejected. The series is not stationary. . As the series is not stationary, the first difference will be taken to remove the impact of the trend. . dickey_fuller(df.diff().dropna()) . ADF Test statistic: -2.314816 p-value: 0.167234 The critical values are: {&#39;1%&#39;: -3.4451307246616514, &#39;5%&#39;: -2.86805689169311, &#39;10%&#39;: -2.570241263729327} The null hypothesis is failed to be rejected. The series is not stationary. . Even after the first difference, the series is not stationarity. The log will first be taken, and then the first difference. . np_df = np.array(df.dropna(), dtype=&#39;float&#39;).squeeze() log_df = np.log(np_df) log_diff_df = np.diff(log_df) dickey_fuller(log_diff_df) . ADF Test statistic: -3.748849 p-value: 0.003476 The critical values are: {&#39;1%&#39;: -3.4452655826028318, &#39;5%&#39;: -2.868116205869215, &#39;10%&#39;: -2.570272878944473} There is a unit root present and the null hypothesis is rejected. The series is stationary. . This gives a unit root and the series is stationary. Taking the log helps stablise the variance and differences help stabilize the mean. . plt.plot(log_diff_df) plt.title(&#39;First Difference of the Log of Household Goods&#39;) . Text(0.5, 1.0, &#39;First Difference of the Log of Household Goods&#39;) . Correlation functions will be used to determine the appropriate lag length. . def acf(x): &#39;&#39;&#39;Plotting the correlation of lag k with lag 0&#39;&#39;&#39; N = x.shape[0] ac = x-x.mean() lag0 = np.dot(ac, ac) lagk = np.correlate(ac, ac, mode=&#39;full&#39;)[N-1:] r = lagk/lag0 return r . plt.title(&#39;ACF&#39;) lags = 20 plt.scatter(x = range(0,lags+1), y=acf(log_diff_df)[:lags+1]) . &lt;matplotlib.collections.PathCollection at 0x2282ee193c8&gt; . Using acf given by statsmodels we get the following prettier graph: . fig, axs = plt.subplots() plot_acf(log_diff_df, ax=axs, lags=20) plt.show() . From the ACF above, lags 5, 12 have significant correlation with the current value and as well will be included in any further lagged process. There doesn&#39;t seem to be a geometric decay (only for past values of itself) and so it make indicate an MA process. . plot_pacf(log_diff_df, lags=20) plt.show() . If the series was an MA process, there would be a geometric decay in the PACF. . df = reset() df = df[1:] df[&#39;log&#39;] = log_diff_df df = df.drop(&#39;Household goods&#39;,axis=1) df = df.rename(columns={&#39;log&#39;:&#39;Household goods&#39;}) . 3. Predictors . Evaluating Predictors . The Mean Squared Forecast Error (MSFE) will be used to evaluate the predictors . $ MSFE = frac{1}{T-h-T0+1} sum_{t=T0}^{T-h}(y_{t+h}-y_{pred t+h})^2$ . 3.1 - The benchmark: Naive Predictor . The naive predictor used will have the following model specification: $y_t$ = $y_{t-1}$ and will be the baseline for any future model comparisons . plot(df, label = &#39;Naive&#39;,baseline=df.shift(1), title=&#39;Retail Household Goods&#39;) def MSFE_base(yhat, model, h=1, df=df, printf=True): MSFE = np.mean(np.square(yhat - df[&#39;Household goods&#39;][h:])) if printf: return &#39;The MSFE of this {} is {:2f}&#39;.format(model, MSFE) else: return MSFE MSFE_base(df.shift(1)[&#39;Household goods&#39;], &#39;Naive Predictor&#39;) . &#39;The MSFE of this Naive Predictor is 0.049694&#39; . msfe_naive = MSFE_base(df.shift(1)[&#39;Household goods&#39;], model = &#39;naive&#39;,printf=False) . 3.2 - AR(5, 12) Predictor . As established in the ACF and PACF above, lags 5 and 12 will be used within an AR model. . $ y_t = beta_1y_{t-5} + beta_2y_{t-12} + epsilon_t$ . df = reset() df = df[1:] df[&#39;log&#39;] = log_diff_df df = df.drop(&#39;Household goods&#39;,axis=1) df = df.rename(columns={&#39;log&#39;:&#39;Household goods&#39;}) . ytminus5 = df.shift(5) ytminus12 = df.shift(12) ytminus24 = df.shift(24) df = df.join(ytminus5, rsuffix=&#39;yt-5&#39;) df = df.join(ytminus12, rsuffix=&#39;yt-12&#39;) df = df.dropna() y = df[&#39;Household goods&#39;] X = df.drop(&#39;Household goods&#39;, axis=1) . def MSFE(X, y, T0, h, plot=False, model=None): T = len(y) syhat = [] for i in range(T0, T-h): temp_y = y[:i] temp_X = X[:i] temp_X = np.array(temp_X, dtype=&#39;float&#39;) beta_hat = (np.linalg.inv((temp_X.T@temp_X)) @ temp_X.T@temp_y) y_hat2 = temp_X @ beta_hat syhat.append(y_hat2[-1]) if plot: return syhat return np.mean(np.square(y[T0+h:]-syhat)) . def dummies(quarter = False, month = False): X = df.reset_index() if quarter: X[&#39;Dates&#39;] = X[&#39;Dates&#39;].dt.quarter if month: X[&#39;Dates&#39;] = X[&#39;Dates&#39;].dt.month y = X[&#39;Household goods&#39;] X = X.drop(&#39;Household goods&#39;, axis=1) X = pd.get_dummies(X[&#39;Dates&#39;]) X.reset_index(inplace=True) return X def testing(X, T0=50, h=1, df = df): syhat = [] for t in range(T0, len(df)-h): y = np.array(df[&#39;Household goods&#39;][1:t]) xt = X[1:t] beta_hat = np.linalg.inv((xt.T@xt)) @ xt.T@y #Now need a y_hat of t+h y_hat = X.iloc[t+h]@np.array(beta_hat) syhat.append(y_hat) return syhat def adding_index(df, yhat, T0=50, h=1): df = df.reset_index() df = df.join(pd.DataFrame(yhat, index=range(T0+h,len(df)))) df.set_index(df[&#39;Dates&#39;], inplace=True) df = df.rename(columns={0:&#39;yhat&#39;}) return df def MSFE_new(syhat, model, T0=50, h=1): syhat = np.array(syhat) ytph = df[&#39;Household goods&#39;][T0+h] msfe = np.mean(np.square(ytph-syhat)) return &#39;The MSFE of {} is {}&#39;.format(model, msfe) . AR = MSFE(X, y, 50, 1, plot=True) . df_new = adding_index(df, AR) df_new = df_new.dropna() . plot(df_new[&#39;yhat&#39;], baseline=df_new[&#39;Household goods&#39;], label=&#39;AR Model&#39;) . model = &#39;Auto Regressive Model&#39; print(&#39;The MSFE of this {} is {:2f}&#39;.format(model, MSFE(X,y,50,1))) . The MSFE of this Auto Regressive Model is 0.036034 . msfe_ar = MSFE(X,y,50,1) . 3.3 Linear Regression - Seasonality Controls . Two model specifications will be used, one to act as a dummy for yearly quarters and the other for the months. . $ S_1: y_t = a_1 + a_1t + sum_{i=1}^{4} alpha_iD_{it} + epsilon_t $ . $ S_2: y_t = a_1t + sum_{i=1}^{12} beta_iM_{it}+ epsilon_t $ . $S_1$: This includes a trend - $t$, and a dummy for all quarters $Di$, where $i$ $ epsilon$ [1,4] . $S_2$: This includes a trend $t$, and a dummy for all months of the year - $M_i$, where $i$ $ epsilon$ [1,12] . X_2 = dummies(quarter=True, month=False) syhat_s2 = testing(X_2) plot(adding_index(df, syhat_s2)[&#39;yhat&#39;], baseline=df[&#39;Household goods&#39;], label=&#39;First Specification&#39;) . MSFE_base(syhat_s2, &#39;First Specification&#39;,df=df[50:]) . &#39;The MSFE of this First Specification is 0.010590&#39; . msfe_s1 = MSFE_base(syhat_s2, &#39;frs&#39;, df = df[50:],printf=False) . X_4 = dummies(quarter=False, month=True) syhat_s4 = testing(X_4) plot(adding_index(df, syhat_s4)[&#39;yhat&#39;], baseline=df[&#39;Household goods&#39;], label=&#39;Second Specification&#39;) . MSFE_base(syhat_s4, &#39;Second Specification&#39;,df=df[50:]) . &#39;The MSFE of this Second Specification is 0.002556&#39; . msfe_s2 = MSFE_base(syhat_s4, &#39;frs&#39;, df = df[50:],printf=False) . 3.4 Holt Winters Smoothing . def HWS(T0, s, alpha, beta, gamma, h, T = len(y)): syhat = [] S = np.zeros((1,T-h))[0] L = np.mean(y[1:s]) b = 0 S[1:s] = y[1:s] - L for t in range(s+1, T-h): newL = alpha * (y[t] - S[t-s]) + (1-alpha) * (L+b) newb = beta * (newL-L) + (1-beta)*b S[t] = gamma * (y[t] - newL) + (1-gamma)*S[t-s] yhat = newL + h*newb + S[t+h-s] L = newL b = newb if t &gt;= T0: syhat.append(yhat) return syhat . y = df[&#39;Household goods&#39;] . hws_syhat = HWS(50, 4, 0.4, 0.4, 0.4, 1) #df_new = adding_index(df, hws_syhat, T0=64) df_new = adding_index(df, hws_syhat, T0=50) df_new = df_new.dropna() df_new = df_new.drop(&#39;Dates&#39;, axis=1) . plot(df_new[&#39;yhat&#39;], baseline=df[&#39;Household goods&#39;], label=&#39;Holt Winters Smoothing&#39;) . MSFE_base(df_new[&#39;yhat&#39;], &#39;Holt Winters Smoothing&#39;,df=df) . &#39;The MSFE of this Holt Winters Smoothing is 0.031991&#39; . def optim_point(T0): alpha = np.arange(0,1,0.1) h = 1 beta = gamma = alpha all_values = [] index = [] temp = [] for i in alpha: for j in beta: for k in gamma: syhat = HWS(50, 4, i, j, k, h) temp = np.mean(np.square(syhat - df[&#39;Household goods&#39;][T0+h:])) index.append((i,j,k)) all_values.append(temp) opt_point = np.array(index)[all_values == np.min(all_values)] return opt_point . optimal = optim_point(50) print(&#39;The optimal parameters for the Holt Winter Smoothing are: n{}&#39;.format(optimal)) . The optimal parameters for the Holt Winter Smoothing are: [[0. 0. 0.1] [0. 0.1 0.1] [0. 0.2 0.1] [0. 0.3 0.1] [0. 0.4 0.1] [0. 0.5 0.1] [0. 0.6 0.1] [0. 0.7 0.1] [0. 0.8 0.1] [0. 0.9 0.1]] . hws_syhat = HWS(T0=50, s=4, alpha=0, beta=0, gamma=0.1, h=1) df_new = adding_index(df, hws_syhat, T0=50) df_new = df_new.dropna() plot(df_new[&#39;yhat&#39;], baseline=df[&#39;Household goods&#39;], label=&#39;Holt Winters Smoothing&#39;) . msfe_hw = MSFE_base(df_new[&#39;yhat&#39;], &#39;Holt Winters Smoothing&#39;,df=df, printf=False) MSFE_base(df_new[&#39;yhat&#39;], &#39;Holt Winters Optimized Smoothing&#39;,df=df) . &#39;The MSFE of this Holt Winters Optimized Smoothing is 0.014540&#39; . 3.5 Adding external variables . The Australian National Accounts: National Income, Expenditure and Product dataset was downloaded from the ABS through the following link: https://www.abs.gov.au/statistics/economy/national-accounts/australian-national-accounts-national-income-expenditure-and-product/latest-release#data-download . It gives information of a broader economic glow in Australia. As this heavily relate to the amount of Household Goods being bought, it was thought that adding these as regressors would aid any future predictions of Household Goods. However, there are 108 potential regressors within this new dataset. Principal Component Analysis (PCA) along with relevant Economic knowledge will be used to determine which regressors to add . df_nat = pd.read_excel(&#39;National_accounts.xls&#39;,sheet_name = &#39;Data1&#39;, index_col=0, prase_dates=True) Xs = df_nat[10:].dropna(axis=1) yXs = Xs.join(df).dropna() . new = (yXs).drop(&#39;Household goods&#39;, axis=1) from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA sc = StandardScaler() X_std = sc.fit_transform(new) . pca = PCA(n_components=0.95) X_pca = pca.fit_transform(X_std) . n_pcs= pca.n_components_ # get number of component # get the index of the most important feature on EACH component most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)] initial_feature_names = new.columns # get the most important feature names most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)] . Through the above PCA analysis, the most relevant columns are the ones below. . most_important_names . [&#39;GDP per capita: Current prices ;.2&#39;, &#39;RGDI&#39;, &#39;Real gross domestic income: Chain volume measures - Percentage changes ;.1&#39;, &#39;Household saving ratio: Ratio ;.2&#39;, &#39;Terms of trade: Index - Percentage changes ;.1&#39;, &#39;Household goodsyt-12&#39;] . Using PCA on the new dataset, the most important variables are: RGDI, terms of trade and household saving ratio. These will be added into future models . The new timeseries have values collected only every 3 months while household goods have values collected every month. Instead of reducing the number of household good observations to match the new timeseries, it was decided to instead interpolate the new timeseries datasets to fill in the gaps. The method used was a simple linear method as it was determined the underlying functions of RGDI, Terms of Trade and Household saving ratio were not too ambitious. . reg = yXs[[&#39;RGDI&#39;, &#39;Terms of trade: Index - Percentage changes ;.1&#39;, &#39;Household saving ratio: Ratio ;.2&#39;]] . y = df[&#39;Household goods&#39;] . all_df = df.join(reg) all_df = all_df.apply(pd.to_numeric) all_df = all_df.interpolate(method=&#39;linear&#39;, limit_direction=&#39;forward&#39;, axis=0)[1:] . all_df = all_df.rename(columns={&#39;Terms of trade: Index - Percentage changes ;.1&#39;:&#39;Terms of trade&#39;, &#39;Household saving ratio: Ratio ;.2&#39;:&#39;Savings ratio&#39;}) new_X = all_df[[&#39;RGDI&#39;,&#39;Terms of trade&#39;,&#39;Savings ratio&#39;]] new_y = pd.DataFrame(all_df[&#39;Household goods&#39;]) . Equation 1: $ y_t = beta_0 + beta_1RGDI + beta_2TOT + beta_3Savings $ . var_syhat = testing(new_X, df=new_y) . plot(adding_index(all_df, var_syhat)[&#39;yhat&#39;], title=&#39;Equation 1&#39;,baseline=(all_df[&#39;Household goods&#39;][51:])) . Equation 2: $ y_t = y_{t-1} beta_0 + beta_1RGDI + beta_2TOT + beta_3Savings $ . AR_X = new_X.join(y.shift(1)) AR_syhat = testing(AR_X, df=new_y) . plot(adding_index(all_df, AR_syhat)[&#39;yhat&#39;], title=&#39;Equation 2&#39;, baseline=(all_df[&#39;Household goods&#39;])) . msfe_ar1 = MSFE_base(AR_syhat, &#39;Linear Regression&#39;,df=df[51:], printf=False) . MSFE_base(AR_syhat, &#39;Linear Regression&#39;,df=df[51:]) . &#39;The MSFE of this Linear Regression is 0.013523&#39; . Equation 3: $ y_t = alpha_0 y_{t-1} + alpha_1RGDI + alpha_2TOT + alpha_3Savings + a_1t + sum_{i=1}^{12} beta_iM_{it} + epsilon_t$ . X_4 = X_4[1:].set_index(all_df.index) . new_X_4 = all_df.join(X_4) new_X_4[[&#39;Household goods&#39;, &#39;Household goodsyt-5&#39;, &#39;Household goodsyt-12&#39;]] = new_X_4[[&#39;Household goods&#39;, &#39;Household goodsyt-5&#39;, &#39;Household goodsyt-12&#39;]].shift(1) new_X_4 = new_X_4[1:] . new_X_4_syhat = testing(new_X_4, df=new_y[1:]) len(new_X_4_syhat) . 394 . plot(adding_index(all_df[1:], new_X_4_syhat)[&#39;yhat&#39;], title = &#39;Equation 3&#39;, baseline=(all_df[&#39;Household goods&#39;][1:])) . msfe_ar2 = MSFE_base(new_X_4_syhat, &#39;Equation 3&#39;,df=df[52:],printf=False) MSFE_base(new_X_4_syhat, &#39;Equation 3&#39;,df=df[52:]) . &#39;The MSFE of this Equation 3 is 0.001929&#39; . 3.6 Vector AutoRegression . dickey_fuller(all_df[&#39;RGDI&#39;]) . ADF Test statistic: -4.014061 p-value: 0.001339 The critical values are: {&#39;1%&#39;: -3.445685337552546, &#39;5%&#39;: -2.868300808913956, &#39;10%&#39;: -2.570371276889389} There is a unit root present and the null hypothesis is rejected. The series is stationary. . dickey_fuller(all_df[&#39;Terms of trade&#39;]) . ADF Test statistic: -3.707819 p-value: 0.004005 The critical values are: {&#39;1%&#39;: -3.445757604526768, &#39;5%&#39;: -2.8683325885102855, &#39;10%&#39;: -2.5703882165206853} There is a unit root present and the null hypothesis is rejected. The series is stationary. . dickey_fuller(all_df[&#39;Savings ratio&#39;]) . ADF Test statistic: -2.778849 p-value: 0.061359 The critical values are: {&#39;1%&#39;: -3.445685337552546, &#39;5%&#39;: -2.868300808913956, &#39;10%&#39;: -2.570371276889389} There is a unit root present and the null hypothesis is rejected. The series is stationary. . All relevant variables [RGDI, Terms of Trade, Savings ratio] are stationary. . $ y_t = b + B_1y_{t-1} + ... + B_py_{t-p} + epsilon_t$ . where $b$ is an n x 1 vector of intercepts and $B_i$ is an n x n coefficient matrix . $$ begin{bmatrix}y_{1,t} y_{2,t} ... y_{p,t} end{bmatrix} = begin{bmatrix}b_1 b_2 ... b_p end{bmatrix} + begin {bmatrix}B_{11,1} &amp; B_{12,2} &amp; B_{13,3} &amp; B_{14,4} B_{21,2} &amp; B_{22,2} &amp; B_{23,3} &amp; B_{24, 4} ... &amp; ... &amp; ... &amp; ... B_{p1,p} &amp; B_{p2,p} &amp; B_{p3,p} &amp; B_{p4,p} end{bmatrix} begin{bmatrix}y_{1,t-1} y_{2,t-2} ... y_{p,t-p} end{bmatrix} + begin{bmatrix} epsilon_{1,t} epsilon_{2,t-2} ... epsilon_{p,p} end{bmatrix} $$ Rewrite our model as $ y = X beta + epsilon $, where $ epsilon ~ N(0, Sigma)$ . $ beta = (X&#39;X)^{-1}X&#39;y $ . $ Sigma = 1/T(y_t - X_t beta)(y_t - X_t beta)&#39; $ . X = all_df[[&#39;Household goods&#39;, &#39;RGDI&#39;, &#39;Terms of trade&#39;, &#39;Savings ratio&#39;]] X[&#39;Household goods&#39;] = X[&#39;Household goods&#39;].shift(1) X = X.rename(columns={&#39;Household goods&#39;: &#39;Household goods -1&#39;}) X = X[1:] y = all_df[&#39;Household goods&#39;][1:] . C: Users idias Anaconda3 lib site-packages ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . Betahat = np.linalg.inv(X.T@X) @ X.T@y Betahat = np.array(Betahat).reshape(-1,1) . T = len(y) T0 = 50 yhatVAR = [] for t in range(T0, T): yt = y[:t] Xt = X[:t] beta = np.array(np.linalg.inv(X.T@X) @ X.T@y).reshape(-1,4) yt = np.array(yt) yt = yt.reshape(-1,1) all_yt = np.concatenate((yt[:t-3], yt[1:t-2], yt[2:t-1], yt[3:t]), axis=1, ).reshape(-1,4) yhatVAR.append((beta @ all_yt.T)[-1][-1]) . msfe_var = np.mean(np.square(y[T0+1:] - yhatVAR[:-1])) print(&#39;The MSFE of the VAR is: &#39;, msfe_var) . The MSFE of the VAR is: 0.02063982035930446 . y = pd.DataFrame(y) y.loc[&#39;2020-09-01 00:00:00&#39;] = yhatVAR[-1] . yhatVARindex = adding_index(df=y, yhat = yhatVAR)[&#39;yhat&#39;] plot(yhatVARindex, baseline=df[&#39;Household goods&#39;], title=&#39;Vector Autoregression&#39;) . 4. Summary . print(&#39;&#39;&#39;Summary of MSFEs from all the models: n nThe Naive model: {:3f} nThe AR(5,12) model: {:3f} nThe Holt winters model: {:3f} The AR and external variable (1) model: {:3f} nThe AR and extgenal variable (2) model: {:3f} The VAR model: {:3f}&#39;&#39;&#39;.format(msfe_naive, msfe_ar, msfe_hw, msfe_ar1, msfe_ar2, msfe_var)) . Summary of MSFEs from all the models: The Naive model: 0.049694 The AR(5,12) model: 0.036034 The Holt winters model: 0.014540 The AR and external variable (1) model: 0.013523 The AR and extgenal variable (2) model: 0.001929 The VAR model: 0.020640 . The model with the lowest MSFE was the following: . $ y_t = alpha_0 y_{t-1} + alpha_1RGDI + alpha_2TOT + alpha_3Savings + a_1t + sum_{i=1}^{12} beta_iM_{it} + epsilon_t$ . 5. Predictions . def testing(X, T0=50, h=1, df = df): syhat = [] for t in range(T0, len(df)-h): y = np.array(df[&#39;Household goods&#39;][1:t]) xt = X[1:t] beta_hat = np.linalg.inv((xt.T@xt)) @ xt.T@y #Now need a y_hat of t+h y_hat = X.iloc[t+h]@np.array(beta_hat) syhat.append(y_hat) return syhat . new_X_4_syhat = testing(new_X_4, df=new_y[1:], h=1) . msfe_ar2 = MSFE_base(new_X_4_syhat, &#39;model&#39;,df=df[52:],printf=False) MSFE_base(new_X_4_syhat, &#39;model&#39;,df=df[52:]) . &#39;The MSFE of this model is 0.001929&#39; . date_index = [&#39;2020-09-01 00:00:00&#39;, &#39;2020-10-01 00:00:00&#39;, &#39;2020-11-01 00:00:00&#39;, &#39;2020-12-01 00:00:00&#39;] h = 4 new_X_4_syhat = testing(new_X_4, df=new_y[1:], h=h) new_x = adding_index(all_df[8:], new_X_4_syhat[:-h]) MSFE_base(new_X_4_syhat[:-h], &#39;model for h=4&#39;,df=df[59:]) #MSFE for when h = 4 . &#39;The MSFE of this model for h=4 is 0.043030&#39; . Point forecasts . h=4 forecast = pd.DataFrame(new_X_4_syhat[-h:],index=date_index[:h]) forecast = forecast.rename(columns={0:&#39;yhat&#39;}) forecast = new_x.append(forecast) forecast.index = pd.to_datetime(forecast.index) . plot(forecast[&#39;yhat&#39;], title=&#39;Forecast for remaining 2020&#39;, baseline=df[&#39;Household goods&#39;], label=&#39;forecast&#39;) . Forecasting 4 periods in the future for the remaining months of 2020 yields the following: . forecast[-h:][&#39;yhat&#39;] . 2020-09-01 0.049552 2020-10-01 -0.071381 2020-11-01 -0.054260 2020-12-01 -0.014781 Name: yhat, dtype: float64 . Now we need to convert it back to the original data values . orig = reset() #This is the original dataset i.e. df[&#39;Household goods&#39;] #Need to make sure the indexes match the predicted yhat, had to use orig[18:] #Forecast[&#39;yhat&#39;] is the predicted yhat. Make sure the indexes are correct real_forecast = np.exp(forecast[&#39;yhat&#39;]).cumsum() + orig[18:].set_index(forecast.index)[&#39;Household goods&#39;] . plot(real_forecast, title = &#39;Predicted forecast&#39;, label=&#39;forecast&#39;,baseline=orig) . The forecast with the original values for the remaining months of 2020 . real_forecast[-h:] . 2020-09-01 6001.99 2020-10-01 6151.92 2020-11-01 6161.16 2020-12-01 5734.15 dtype: object . Density Forecasts . k = 6 orig = reset() var = 1/(len(orig)-k) * sum(np.square((real_forecast.dropna() - orig[69:][&#39;Household goods&#39;]).dropna())) . real_density = [] for i in real_forecast[-h:]: real_density.append((i + 1.96*np.sqrt(var), i - 1.96*np.sqrt(var))) . print(&#39;&#39;&#39;On 2020-09-01, there is a 95% probability of the data lying between {} and {} On 2020-10-01, there is a 95% probability of the data lying between {} and {} On 2020-11-01, there is a 95% probability of the data lying between {} and {} On 2020-12-01, there is a 95% probability of the data lying between {} and {}&#39;&#39;&#39; .format(real_density[0][1],real_density[0][0], real_density[1][1],real_density[1][0], real_density[2][1],real_density[2][0], real_density[3][1],real_density[3][0])) . On 2020-09-01, there is a 95% probability of the data lying between 4971.335179432508 and 7032.635330243136 On 2020-10-01, there is a 95% probability of the data lying between 5121.266286546312 and 7182.566437356938 On 2020-11-01, there is a 95% probability of the data lying between 5130.513471890277 and 7191.813622700904 On 2020-12-01, there is a 95% probability of the data lying between 4703.498799674406 and 6764.798950485032 . The following is a one-step ahead forecast, instead of a 4-step forecast for the entire year. . One-step ahead forecast for evaluation . orig = reset() h=1 forecast = pd.DataFrame(new_X_4_syhat[-h:],index=date_index[:h]) forecast = forecast.rename(columns={0:&#39;yhat&#39;}) forecast = new_x.append(forecast) forecast.index = pd.to_datetime(forecast.index) orig = reset() real_forecast = np.exp(forecast[&#39;yhat&#39;]).cumsum() + orig[21:].set_index(forecast.index)[&#39;Household goods&#39;] real_forecast[-h:] . 2020-09-01 5731.22 dtype: object . k = 6 orig = reset() var = 1/(len(orig)-k) * sum(np.square((real_forecast.dropna() - orig[69:][&#39;Household goods&#39;]).dropna())) real_density = [] for i in real_forecast[-h:]: real_density.append((i + 1.96*np.sqrt(var), i - 1.96*np.sqrt(var))) print(&#39;On 2020-09-01, there is a 95% probability of the data lying between {} and {}&#39; .format(real_density[0][1],real_density[0][0])) . On 2020-09-01, there is a 95% probability of the data lying between 4916.835561142746 and 6545.604004096039 . reset(&#39;new_data.xls&#39;) . Household goods . Dates . 1982-05-01 629.6 | . 1982-06-01 607.4 | . 1982-07-01 632.4 | . 1982-08-01 622.6 | . 1982-09-01 622 | . ... ... | . 2020-05-01 5607.6 | . 2020-06-01 5756.6 | . 2020-07-01 5764.9 | . 2020-08-01 5336.9 | . 2020-09-01 5272.7 | . 461 rows × 1 columns .",
            "url": "https://ianldias.github.io/Portfolio/2020/11/15/Forecasting.html",
            "relUrl": "/2020/11/15/Forecasting.html",
            "date": " • Nov 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ianldias.github.io/Portfolio/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ianldias.github.io/Portfolio/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ianldias.github.io/Portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ianldias.github.io/Portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}