{
  
    
        "post0": {
            "title": "Forecasting retail turnover of household goods in the Australian economy",
            "content": "Ian Dias . Nov 2020 . Contents . 1. Visualization of the Time-Series . 2. Stationarity Checks . 3. Predictors . 3.1 Naive Predictor . 3.2 AR Predictor . 3.3 Linear Regression - Seasonality Controls . 3.4 Holt Winters Smoothing . 3.5 Adding external variables . 3.6 Vector AutoRegression . 4. Summary . 5. Predictions . Introduction . I plan to forecast the monthly retail turnover of household goods in the Australian wide economy. The ABS has collected this data from May of 1982 and the last data available is until August 2020. The amount of household goods being traded is an important indication of an economy - as the goods range from electrical, to househod building and furniture. It appears that the more individuals are able to spend on these goods the seemingly more satisfaction and utility they achieve. . Due to the recent impacts of the COVID-19 pandemic, it would be of interest how this has affected the household goods industry. The Australian economy as a whole has reduced, but given that consumers are spending more time at home the impact it has had on this specific industry might not be completely obvious. . The data is accessed from: https://www.abs.gov.au/statistics/industry/retail-and-wholesale-trade/retail-trade-australia/latest-release . 1. Visualization of the time-series . import pandas as pd from matplotlib import pyplot as plt import seaborn as sns import numpy as np from statsmodels.graphics.tsaplots import plot_acf from statsmodels.graphics.tsaplots import plot_pacf sns.set_style(&#39;darkgrid&#39;) plt.rcParams[&quot;figure.figsize&quot;] = (10,5) . def reset(data=&#39;850101.xls&#39;): df = pd.read_excel(data,usecols=[0,2],sheet_name = &#39;Data1&#39;,skiprows=&#39;5&#39;) df = df[10:] df.set_index(&#39;Unnamed: 0&#39;, inplace=True) df.index.name = &#39;Dates&#39; df.columns = [&#39;Household goods&#39;] df.index = pd.to_datetime(df.index) return df df = reset() . def plot(x,title=None,baseline=None, **kwargs): plt.plot(x, **kwargs) if baseline is not None: plt.plot(baseline, label=&#39;Baseline&#39;, alpha=0.5) plt.legend() plt.title(title) . plot(df, title=&#39;Retail Household Goods&#39;) . There seems to be both clear seasonality and and upwards trend to the data. The data will be isolated by year to identify the seasonality patterns and the impacts of the trend. . Visualizing the data and patterns . df[&#39;year&#39;] = df.index.year df[&#39;month&#39;] = df.index.month years = df[&#39;year&#39;].unique() for i, y in enumerate(years): plt.plot(&#39;month&#39;, &#39;Household goods&#39;, data=df[df[&#39;year&#39;] == y], label=y) if i % 4 == 0 or y == 2020: plt.text(df[df[&#39;year&#39;] == y].shape[0], df[df[&#39;year&#39;] == y][&#39;Household goods&#39;][-1:].values[0], y) . Looking at the above graph, it appears that there is a definite upward tick for December and there could possibly be monthly or quarterly variation in the data.The mean also appears to be higher every year. There seems to be a significant increase in spending in 2020 starting from February and going on until the last value of October. This is most likely due to the COVID-19 virus that is impacting the world and began its Australian penetration in March. As more individuals are under lockdown they are staying home longer and this may increase the quantity of household goods they would typically require. . 2. Stationarity checks . df = reset() . from statsmodels.tsa.stattools import adfuller first_diff = df.diff() def dickey_fuller(x): &#39;&#39;&#39;Dickey-fuller test to check for stationarity&#39;&#39;&#39; result = adfuller(x) print(&#39;ADF Test statistic: %f&#39; % result[0]) print(&#39;p-value: %f&#39; % result[1]) print(&#39;The critical values are: {}&#39; .format(result[4])) for key,value in result[4].items(): if value &gt; result[0]: print(&#39;There is a unit root present and the null hypothesis is rejected. The series is stationary.&#39;) return print(&#39;The null hypothesis is failed to be rejected. The series is not stationary.&#39;) dickey_fuller(df) . ADF Test statistic: 1.572934 p-value: 0.997774 The critical values are: {&#39;1%&#39;: -3.4451307246616514, &#39;5%&#39;: -2.86805689169311, &#39;10%&#39;: -2.570241263729327} The null hypothesis is failed to be rejected. The series is not stationary. . As the series is not stationary, the first difference will be taken to remove the impact of the trend. . dickey_fuller(df.diff().dropna()) . ADF Test statistic: -2.314816 p-value: 0.167234 The critical values are: {&#39;1%&#39;: -3.4451307246616514, &#39;5%&#39;: -2.86805689169311, &#39;10%&#39;: -2.570241263729327} The null hypothesis is failed to be rejected. The series is not stationary. . Even after the first difference, the series is not stationarity. The log will first be taken, and then the first difference. . np_df = np.array(df.dropna(), dtype=&#39;float&#39;).squeeze() log_df = np.log(np_df) log_diff_df = np.diff(log_df) dickey_fuller(log_diff_df) . ADF Test statistic: -3.748849 p-value: 0.003476 The critical values are: {&#39;1%&#39;: -3.4452655826028318, &#39;5%&#39;: -2.868116205869215, &#39;10%&#39;: -2.570272878944473} There is a unit root present and the null hypothesis is rejected. The series is stationary. . This gives a unit root and the series is stationary. Taking the log helps stablise the variance and differences help stabilize the mean. . plt.plot(log_diff_df) plt.title(&#39;First Difference of the Log of Household Goods&#39;) . Text(0.5, 1.0, &#39;First Difference of the Log of Household Goods&#39;) . Correlation functions will be used to determine the appropriate lag length. . def acf(x): &#39;&#39;&#39;Plotting the correlation of lag k with lag 0&#39;&#39;&#39; N = x.shape[0] ac = x-x.mean() lag0 = np.dot(ac, ac) lagk = np.correlate(ac, ac, mode=&#39;full&#39;)[N-1:] r = lagk/lag0 return r . plt.title(&#39;ACF&#39;) lags = 20 plt.scatter(x = range(0,lags+1), y=acf(log_diff_df)[:lags+1]) . &lt;matplotlib.collections.PathCollection at 0x2282ee193c8&gt; . Using acf given by statsmodels we get the following prettier graph: . fig, axs = plt.subplots() plot_acf(log_diff_df, ax=axs, lags=20) plt.show() . From the ACF above, lags 5, 12 have significant correlation with the current value and as well will be included in any further lagged process. There doesn&#39;t seem to be a geometric decay (only for past values of itself) and so it make indicate an MA process. . plot_pacf(log_diff_df, lags=20) plt.show() . If the series was an MA process, there would be a geometric decay in the PACF. . df = reset() df = df[1:] df[&#39;log&#39;] = log_diff_df df = df.drop(&#39;Household goods&#39;,axis=1) df = df.rename(columns={&#39;log&#39;:&#39;Household goods&#39;}) . 3. Predictors . Evaluating Predictors . The Mean Squared Forecast Error (MSFE) will be used to evaluate the predictors . $ MSFE = frac{1}{T-h-T0+1} sum_{t=T0}^{T-h}(y_{t+h}-y_{pred t+h})^2$ . 3.1 - The benchmark: Naive Predictor . The naive predictor used will have the following model specification: $y_t$ = $y_{t-1}$ and will be the baseline for any future model comparisons . plot(df, label = &#39;Naive&#39;,baseline=df.shift(1), title=&#39;Retail Household Goods&#39;) def MSFE_base(yhat, model, h=1, df=df, printf=True): MSFE = np.mean(np.square(yhat - df[&#39;Household goods&#39;][h:])) if printf: return &#39;The MSFE of this {} is {:2f}&#39;.format(model, MSFE) else: return MSFE MSFE_base(df.shift(1)[&#39;Household goods&#39;], &#39;Naive Predictor&#39;) . &#39;The MSFE of this Naive Predictor is 0.049694&#39; . msfe_naive = MSFE_base(df.shift(1)[&#39;Household goods&#39;], model = &#39;naive&#39;,printf=False) . 3.2 - AR(5, 12) Predictor . As established in the ACF and PACF above, lags 5 and 12 will be used within an AR model. . $ y_t = beta_1y_{t-5} + beta_2y_{t-12} + epsilon_t$ . df = reset() df = df[1:] df[&#39;log&#39;] = log_diff_df df = df.drop(&#39;Household goods&#39;,axis=1) df = df.rename(columns={&#39;log&#39;:&#39;Household goods&#39;}) . ytminus5 = df.shift(5) ytminus12 = df.shift(12) ytminus24 = df.shift(24) df = df.join(ytminus5, rsuffix=&#39;yt-5&#39;) df = df.join(ytminus12, rsuffix=&#39;yt-12&#39;) df = df.dropna() y = df[&#39;Household goods&#39;] X = df.drop(&#39;Household goods&#39;, axis=1) . def MSFE(X, y, T0, h, plot=False, model=None): T = len(y) syhat = [] for i in range(T0, T-h): temp_y = y[:i] temp_X = X[:i] temp_X = np.array(temp_X, dtype=&#39;float&#39;) beta_hat = (np.linalg.inv((temp_X.T@temp_X)) @ temp_X.T@temp_y) y_hat2 = temp_X @ beta_hat syhat.append(y_hat2[-1]) if plot: return syhat return np.mean(np.square(y[T0+h:]-syhat)) . def dummies(quarter = False, month = False): X = df.reset_index() if quarter: X[&#39;Dates&#39;] = X[&#39;Dates&#39;].dt.quarter if month: X[&#39;Dates&#39;] = X[&#39;Dates&#39;].dt.month y = X[&#39;Household goods&#39;] X = X.drop(&#39;Household goods&#39;, axis=1) X = pd.get_dummies(X[&#39;Dates&#39;]) X.reset_index(inplace=True) return X def testing(X, T0=50, h=1, df = df): syhat = [] for t in range(T0, len(df)-h): y = np.array(df[&#39;Household goods&#39;][1:t]) xt = X[1:t] beta_hat = np.linalg.inv((xt.T@xt)) @ xt.T@y #Now need a y_hat of t+h y_hat = X.iloc[t+h]@np.array(beta_hat) syhat.append(y_hat) return syhat def adding_index(df, yhat, T0=50, h=1): df = df.reset_index() df = df.join(pd.DataFrame(yhat, index=range(T0+h,len(df)))) df.set_index(df[&#39;Dates&#39;], inplace=True) df = df.rename(columns={0:&#39;yhat&#39;}) return df def MSFE_new(syhat, model, T0=50, h=1): syhat = np.array(syhat) ytph = df[&#39;Household goods&#39;][T0+h] msfe = np.mean(np.square(ytph-syhat)) return &#39;The MSFE of {} is {}&#39;.format(model, msfe) . AR = MSFE(X, y, 50, 1, plot=True) . df_new = adding_index(df, AR) df_new = df_new.dropna() . plot(df_new[&#39;yhat&#39;], baseline=df_new[&#39;Household goods&#39;], label=&#39;AR Model&#39;) . model = &#39;Auto Regressive Model&#39; print(&#39;The MSFE of this {} is {:2f}&#39;.format(model, MSFE(X,y,50,1))) . The MSFE of this Auto Regressive Model is 0.036034 . msfe_ar = MSFE(X,y,50,1) . 3.3 Linear Regression - Seasonality Controls . Two model specifications will be used, one to act as a dummy for yearly quarters and the other for the months. . $ S_1: y_t = a_1 + a_1t + sum_{i=1}^{4} alpha_iD_{it} + epsilon_t $ . $ S_2: y_t = a_1t + sum_{i=1}^{12} beta_iM_{it}+ epsilon_t $ . $S_1$: This includes a trend - $t$, and a dummy for all quarters $Di$, where $i$ $ epsilon$ [1,4] . $S_2$: This includes a trend $t$, and a dummy for all months of the year - $M_i$, where $i$ $ epsilon$ [1,12] . X_2 = dummies(quarter=True, month=False) syhat_s2 = testing(X_2) plot(adding_index(df, syhat_s2)[&#39;yhat&#39;], baseline=df[&#39;Household goods&#39;], label=&#39;First Specification&#39;) . MSFE_base(syhat_s2, &#39;First Specification&#39;,df=df[50:]) . &#39;The MSFE of this First Specification is 0.010590&#39; . msfe_s1 = MSFE_base(syhat_s2, &#39;frs&#39;, df = df[50:],printf=False) . X_4 = dummies(quarter=False, month=True) syhat_s4 = testing(X_4) plot(adding_index(df, syhat_s4)[&#39;yhat&#39;], baseline=df[&#39;Household goods&#39;], label=&#39;Second Specification&#39;) . MSFE_base(syhat_s4, &#39;Second Specification&#39;,df=df[50:]) . &#39;The MSFE of this Second Specification is 0.002556&#39; . msfe_s2 = MSFE_base(syhat_s4, &#39;frs&#39;, df = df[50:],printf=False) . 3.4 Holt Winters Smoothing . def HWS(T0, s, alpha, beta, gamma, h, T = len(y)): syhat = [] S = np.zeros((1,T-h))[0] L = np.mean(y[1:s]) b = 0 S[1:s] = y[1:s] - L for t in range(s+1, T-h): newL = alpha * (y[t] - S[t-s]) + (1-alpha) * (L+b) newb = beta * (newL-L) + (1-beta)*b S[t] = gamma * (y[t] - newL) + (1-gamma)*S[t-s] yhat = newL + h*newb + S[t+h-s] L = newL b = newb if t &gt;= T0: syhat.append(yhat) return syhat . y = df[&#39;Household goods&#39;] . hws_syhat = HWS(50, 4, 0.4, 0.4, 0.4, 1) #df_new = adding_index(df, hws_syhat, T0=64) df_new = adding_index(df, hws_syhat, T0=50) df_new = df_new.dropna() df_new = df_new.drop(&#39;Dates&#39;, axis=1) . plot(df_new[&#39;yhat&#39;], baseline=df[&#39;Household goods&#39;], label=&#39;Holt Winters Smoothing&#39;) . MSFE_base(df_new[&#39;yhat&#39;], &#39;Holt Winters Smoothing&#39;,df=df) . &#39;The MSFE of this Holt Winters Smoothing is 0.031991&#39; . def optim_point(T0): alpha = np.arange(0,1,0.1) h = 1 beta = gamma = alpha all_values = [] index = [] temp = [] for i in alpha: for j in beta: for k in gamma: syhat = HWS(50, 4, i, j, k, h) temp = np.mean(np.square(syhat - df[&#39;Household goods&#39;][T0+h:])) index.append((i,j,k)) all_values.append(temp) opt_point = np.array(index)[all_values == np.min(all_values)] return opt_point . optimal = optim_point(50) print(&#39;The optimal parameters for the Holt Winter Smoothing are: n{}&#39;.format(optimal)) . The optimal parameters for the Holt Winter Smoothing are: [[0. 0. 0.1] [0. 0.1 0.1] [0. 0.2 0.1] [0. 0.3 0.1] [0. 0.4 0.1] [0. 0.5 0.1] [0. 0.6 0.1] [0. 0.7 0.1] [0. 0.8 0.1] [0. 0.9 0.1]] . hws_syhat = HWS(T0=50, s=4, alpha=0, beta=0, gamma=0.1, h=1) df_new = adding_index(df, hws_syhat, T0=50) df_new = df_new.dropna() plot(df_new[&#39;yhat&#39;], baseline=df[&#39;Household goods&#39;], label=&#39;Holt Winters Smoothing&#39;) . msfe_hw = MSFE_base(df_new[&#39;yhat&#39;], &#39;Holt Winters Smoothing&#39;,df=df, printf=False) MSFE_base(df_new[&#39;yhat&#39;], &#39;Holt Winters Optimized Smoothing&#39;,df=df) . &#39;The MSFE of this Holt Winters Optimized Smoothing is 0.014540&#39; . 3.5 Adding external variables . The Australian National Accounts: National Income, Expenditure and Product dataset was downloaded from the ABS through the following link: https://www.abs.gov.au/statistics/economy/national-accounts/australian-national-accounts-national-income-expenditure-and-product/latest-release#data-download . It gives information of a broader economic glow in Australia. As this heavily relate to the amount of Household Goods being bought, it was thought that adding these as regressors would aid any future predictions of Household Goods. However, there are 108 potential regressors within this new dataset. Principal Component Analysis (PCA) along with relevant Economic knowledge will be used to determine which regressors to add . df_nat = pd.read_excel(&#39;National_accounts.xls&#39;,sheet_name = &#39;Data1&#39;, index_col=0, prase_dates=True) Xs = df_nat[10:].dropna(axis=1) yXs = Xs.join(df).dropna() . new = (yXs).drop(&#39;Household goods&#39;, axis=1) from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA sc = StandardScaler() X_std = sc.fit_transform(new) . pca = PCA(n_components=0.95) X_pca = pca.fit_transform(X_std) . n_pcs= pca.n_components_ # get number of component # get the index of the most important feature on EACH component most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)] initial_feature_names = new.columns # get the most important feature names most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)] . Through the above PCA analysis, the most relevant columns are the ones below. . most_important_names . [&#39;GDP per capita: Current prices ;.2&#39;, &#39;RGDI&#39;, &#39;Real gross domestic income: Chain volume measures - Percentage changes ;.1&#39;, &#39;Household saving ratio: Ratio ;.2&#39;, &#39;Terms of trade: Index - Percentage changes ;.1&#39;, &#39;Household goodsyt-12&#39;] . Using PCA on the new dataset, the most important variables are: RGDI, terms of trade and household saving ratio. These will be added into future models . The new timeseries have values collected only every 3 months while household goods have values collected every month. Instead of reducing the number of household good observations to match the new timeseries, it was decided to instead interpolate the new timeseries datasets to fill in the gaps. The method used was a simple linear method as it was determined the underlying functions of RGDI, Terms of Trade and Household saving ratio were not too ambitious. . reg = yXs[[&#39;RGDI&#39;, &#39;Terms of trade: Index - Percentage changes ;.1&#39;, &#39;Household saving ratio: Ratio ;.2&#39;]] . y = df[&#39;Household goods&#39;] . all_df = df.join(reg) all_df = all_df.apply(pd.to_numeric) all_df = all_df.interpolate(method=&#39;linear&#39;, limit_direction=&#39;forward&#39;, axis=0)[1:] . all_df = all_df.rename(columns={&#39;Terms of trade: Index - Percentage changes ;.1&#39;:&#39;Terms of trade&#39;, &#39;Household saving ratio: Ratio ;.2&#39;:&#39;Savings ratio&#39;}) new_X = all_df[[&#39;RGDI&#39;,&#39;Terms of trade&#39;,&#39;Savings ratio&#39;]] new_y = pd.DataFrame(all_df[&#39;Household goods&#39;]) . Equation 1: $ y_t = beta_0 + beta_1RGDI + beta_2TOT + beta_3Savings $ . var_syhat = testing(new_X, df=new_y) . plot(adding_index(all_df, var_syhat)[&#39;yhat&#39;], title=&#39;Equation 1&#39;,baseline=(all_df[&#39;Household goods&#39;][51:])) . Equation 2: $ y_t = y_{t-1} beta_0 + beta_1RGDI + beta_2TOT + beta_3Savings $ . AR_X = new_X.join(y.shift(1)) AR_syhat = testing(AR_X, df=new_y) . plot(adding_index(all_df, AR_syhat)[&#39;yhat&#39;], title=&#39;Equation 2&#39;, baseline=(all_df[&#39;Household goods&#39;])) . msfe_ar1 = MSFE_base(AR_syhat, &#39;Linear Regression&#39;,df=df[51:], printf=False) . MSFE_base(AR_syhat, &#39;Linear Regression&#39;,df=df[51:]) . &#39;The MSFE of this Linear Regression is 0.013523&#39; . Equation 3: $ y_t = alpha_0 y_{t-1} + alpha_1RGDI + alpha_2TOT + alpha_3Savings + a_1t + sum_{i=1}^{12} beta_iM_{it} + epsilon_t$ . X_4 = X_4[1:].set_index(all_df.index) . new_X_4 = all_df.join(X_4) new_X_4[[&#39;Household goods&#39;, &#39;Household goodsyt-5&#39;, &#39;Household goodsyt-12&#39;]] = new_X_4[[&#39;Household goods&#39;, &#39;Household goodsyt-5&#39;, &#39;Household goodsyt-12&#39;]].shift(1) new_X_4 = new_X_4[1:] . new_X_4_syhat = testing(new_X_4, df=new_y[1:]) len(new_X_4_syhat) . 394 . plot(adding_index(all_df[1:], new_X_4_syhat)[&#39;yhat&#39;], title = &#39;Equation 3&#39;, baseline=(all_df[&#39;Household goods&#39;][1:])) . msfe_ar2 = MSFE_base(new_X_4_syhat, &#39;Equation 3&#39;,df=df[52:],printf=False) MSFE_base(new_X_4_syhat, &#39;Equation 3&#39;,df=df[52:]) . &#39;The MSFE of this Equation 3 is 0.001929&#39; . 3.6 Vector AutoRegression . dickey_fuller(all_df[&#39;RGDI&#39;]) . ADF Test statistic: -4.014061 p-value: 0.001339 The critical values are: {&#39;1%&#39;: -3.445685337552546, &#39;5%&#39;: -2.868300808913956, &#39;10%&#39;: -2.570371276889389} There is a unit root present and the null hypothesis is rejected. The series is stationary. . dickey_fuller(all_df[&#39;Terms of trade&#39;]) . ADF Test statistic: -3.707819 p-value: 0.004005 The critical values are: {&#39;1%&#39;: -3.445757604526768, &#39;5%&#39;: -2.8683325885102855, &#39;10%&#39;: -2.5703882165206853} There is a unit root present and the null hypothesis is rejected. The series is stationary. . dickey_fuller(all_df[&#39;Savings ratio&#39;]) . ADF Test statistic: -2.778849 p-value: 0.061359 The critical values are: {&#39;1%&#39;: -3.445685337552546, &#39;5%&#39;: -2.868300808913956, &#39;10%&#39;: -2.570371276889389} There is a unit root present and the null hypothesis is rejected. The series is stationary. . All relevant variables [RGDI, Terms of Trade, Savings ratio] are stationary. . $ y_t = b + B_1y_{t-1} + ... + B_py_{t-p} + epsilon_t$ . where $b$ is an n x 1 vector of intercepts and $B_i$ is an n x n coefficient matrix . $$ begin{bmatrix}y_{1,t} y_{2,t} ... y_{p,t} end{bmatrix} = begin{bmatrix}b_1 b_2 ... b_p end{bmatrix} + begin {bmatrix}B_{11,1} &amp; B_{12,2} &amp; B_{13,3} &amp; B_{14,4} B_{21,2} &amp; B_{22,2} &amp; B_{23,3} &amp; B_{24, 4} ... &amp; ... &amp; ... &amp; ... B_{p1,p} &amp; B_{p2,p} &amp; B_{p3,p} &amp; B_{p4,p} end{bmatrix} begin{bmatrix}y_{1,t-1} y_{2,t-2} ... y_{p,t-p} end{bmatrix} + begin{bmatrix} epsilon_{1,t} epsilon_{2,t-2} ... epsilon_{p,p} end{bmatrix} $$ Rewrite our model as $ y = X beta + epsilon $, where $ epsilon ~ N(0, Sigma)$ . $ beta = (X&#39;X)^{-1}X&#39;y $ . $ Sigma = 1/T(y_t - X_t beta)(y_t - X_t beta)&#39; $ . X = all_df[[&#39;Household goods&#39;, &#39;RGDI&#39;, &#39;Terms of trade&#39;, &#39;Savings ratio&#39;]] X[&#39;Household goods&#39;] = X[&#39;Household goods&#39;].shift(1) X = X.rename(columns={&#39;Household goods&#39;: &#39;Household goods -1&#39;}) X = X[1:] y = all_df[&#39;Household goods&#39;][1:] . C: Users idias Anaconda3 lib site-packages ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . Betahat = np.linalg.inv(X.T@X) @ X.T@y Betahat = np.array(Betahat).reshape(-1,1) . T = len(y) T0 = 50 yhatVAR = [] for t in range(T0, T): yt = y[:t] Xt = X[:t] beta = np.array(np.linalg.inv(X.T@X) @ X.T@y).reshape(-1,4) yt = np.array(yt) yt = yt.reshape(-1,1) all_yt = np.concatenate((yt[:t-3], yt[1:t-2], yt[2:t-1], yt[3:t]), axis=1, ).reshape(-1,4) yhatVAR.append((beta @ all_yt.T)[-1][-1]) . msfe_var = np.mean(np.square(y[T0+1:] - yhatVAR[:-1])) print(&#39;The MSFE of the VAR is: &#39;, msfe_var) . The MSFE of the VAR is: 0.02063982035930446 . y = pd.DataFrame(y) y.loc[&#39;2020-09-01 00:00:00&#39;] = yhatVAR[-1] . yhatVARindex = adding_index(df=y, yhat = yhatVAR)[&#39;yhat&#39;] plot(yhatVARindex, baseline=df[&#39;Household goods&#39;], title=&#39;Vector Autoregression&#39;) . 4. Summary . print(&#39;&#39;&#39;Summary of MSFEs from all the models: n nThe Naive model: {:3f} nThe AR(5,12) model: {:3f} nThe Holt winters model: {:3f} The AR and external variable (1) model: {:3f} nThe AR and extgenal variable (2) model: {:3f} The VAR model: {:3f}&#39;&#39;&#39;.format(msfe_naive, msfe_ar, msfe_hw, msfe_ar1, msfe_ar2, msfe_var)) . Summary of MSFEs from all the models: The Naive model: 0.049694 The AR(5,12) model: 0.036034 The Holt winters model: 0.014540 The AR and external variable (1) model: 0.013523 The AR and extgenal variable (2) model: 0.001929 The VAR model: 0.020640 . The model with the lowest MSFE was the following: . $ y_t = alpha_0 y_{t-1} + alpha_1RGDI + alpha_2TOT + alpha_3Savings + a_1t + sum_{i=1}^{12} beta_iM_{it} + epsilon_t$ . 5. Predictions . def testing(X, T0=50, h=1, df = df): syhat = [] for t in range(T0, len(df)-h): y = np.array(df[&#39;Household goods&#39;][1:t]) xt = X[1:t] beta_hat = np.linalg.inv((xt.T@xt)) @ xt.T@y #Now need a y_hat of t+h y_hat = X.iloc[t+h]@np.array(beta_hat) syhat.append(y_hat) return syhat . new_X_4_syhat = testing(new_X_4, df=new_y[1:], h=1) . msfe_ar2 = MSFE_base(new_X_4_syhat, &#39;model&#39;,df=df[52:],printf=False) MSFE_base(new_X_4_syhat, &#39;model&#39;,df=df[52:]) . &#39;The MSFE of this model is 0.001929&#39; . date_index = [&#39;2020-09-01 00:00:00&#39;, &#39;2020-10-01 00:00:00&#39;, &#39;2020-11-01 00:00:00&#39;, &#39;2020-12-01 00:00:00&#39;] h = 4 new_X_4_syhat = testing(new_X_4, df=new_y[1:], h=h) new_x = adding_index(all_df[8:], new_X_4_syhat[:-h]) MSFE_base(new_X_4_syhat[:-h], &#39;model for h=4&#39;,df=df[59:]) #MSFE for when h = 4 . &#39;The MSFE of this model for h=4 is 0.043030&#39; . Point forecasts . h=4 forecast = pd.DataFrame(new_X_4_syhat[-h:],index=date_index[:h]) forecast = forecast.rename(columns={0:&#39;yhat&#39;}) forecast = new_x.append(forecast) forecast.index = pd.to_datetime(forecast.index) . plot(forecast[&#39;yhat&#39;], title=&#39;Forecast for remaining 2020&#39;, baseline=df[&#39;Household goods&#39;], label=&#39;forecast&#39;) . Forecasting 4 periods in the future for the remaining months of 2020 yields the following: . forecast[-h:][&#39;yhat&#39;] . 2020-09-01 0.049552 2020-10-01 -0.071381 2020-11-01 -0.054260 2020-12-01 -0.014781 Name: yhat, dtype: float64 . Now we need to convert it back to the original data values . orig = reset() #This is the original dataset i.e. df[&#39;Household goods&#39;] #Need to make sure the indexes match the predicted yhat, had to use orig[18:] #Forecast[&#39;yhat&#39;] is the predicted yhat. Make sure the indexes are correct real_forecast = np.exp(forecast[&#39;yhat&#39;]).cumsum() + orig[18:].set_index(forecast.index)[&#39;Household goods&#39;] . plot(real_forecast, title = &#39;Predicted forecast&#39;, label=&#39;forecast&#39;,baseline=orig) . The forecast with the original values for the remaining months of 2020 . real_forecast[-h:] . 2020-09-01 6001.99 2020-10-01 6151.92 2020-11-01 6161.16 2020-12-01 5734.15 dtype: object . Density Forecasts . k = 6 orig = reset() var = 1/(len(orig)-k) * sum(np.square((real_forecast.dropna() - orig[69:][&#39;Household goods&#39;]).dropna())) . real_density = [] for i in real_forecast[-h:]: real_density.append((i + 1.96*np.sqrt(var), i - 1.96*np.sqrt(var))) . print(&#39;&#39;&#39;On 2020-09-01, there is a 95% probability of the data lying between {} and {} On 2020-10-01, there is a 95% probability of the data lying between {} and {} On 2020-11-01, there is a 95% probability of the data lying between {} and {} On 2020-12-01, there is a 95% probability of the data lying between {} and {}&#39;&#39;&#39; .format(real_density[0][1],real_density[0][0], real_density[1][1],real_density[1][0], real_density[2][1],real_density[2][0], real_density[3][1],real_density[3][0])) . On 2020-09-01, there is a 95% probability of the data lying between 4971.335179432508 and 7032.635330243136 On 2020-10-01, there is a 95% probability of the data lying between 5121.266286546312 and 7182.566437356938 On 2020-11-01, there is a 95% probability of the data lying between 5130.513471890277 and 7191.813622700904 On 2020-12-01, there is a 95% probability of the data lying between 4703.498799674406 and 6764.798950485032 . The following is a one-step ahead forecast, instead of a 4-step forecast for the entire year. . One-step ahead forecast for evaluation . orig = reset() h=1 forecast = pd.DataFrame(new_X_4_syhat[-h:],index=date_index[:h]) forecast = forecast.rename(columns={0:&#39;yhat&#39;}) forecast = new_x.append(forecast) forecast.index = pd.to_datetime(forecast.index) orig = reset() real_forecast = np.exp(forecast[&#39;yhat&#39;]).cumsum() + orig[21:].set_index(forecast.index)[&#39;Household goods&#39;] real_forecast[-h:] . 2020-09-01 5731.22 dtype: object . k = 6 orig = reset() var = 1/(len(orig)-k) * sum(np.square((real_forecast.dropna() - orig[69:][&#39;Household goods&#39;]).dropna())) real_density = [] for i in real_forecast[-h:]: real_density.append((i + 1.96*np.sqrt(var), i - 1.96*np.sqrt(var))) print(&#39;On 2020-09-01, there is a 95% probability of the data lying between {} and {}&#39; .format(real_density[0][1],real_density[0][0])) . On 2020-09-01, there is a 95% probability of the data lying between 4916.835561142746 and 6545.604004096039 . reset(&#39;new_data.xls&#39;) . Household goods . Dates . 1982-05-01 629.6 | . 1982-06-01 607.4 | . 1982-07-01 632.4 | . 1982-08-01 622.6 | . 1982-09-01 622 | . ... ... | . 2020-05-01 5607.6 | . 2020-06-01 5756.6 | . 2020-07-01 5764.9 | . 2020-08-01 5336.9 | . 2020-09-01 5272.7 | . 461 rows × 1 columns .",
            "url": "https://ianldias.github.io/Portfolio/2020/11/15/Forecasting.html",
            "relUrl": "/2020/11/15/Forecasting.html",
            "date": " • Nov 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Linear Programming in Economics",
            "content": "import numpy as np import matplotlib.pyplot as plt import unittest import seaborn as sns sns.set_style(&#39;darkgrid&#39;) from sympy import symbols, solve from scipy.optimize import linprog . This is an example of a product portfolio choice problem optimizing the following problem of which a firm is able to produce n different products subject to a set of linear technological constraints and which maximizes a linear profit function: . $$ max( pi x) text{ subject to } Ax le b, $$where $ pi $ denotes the vector of per product unit profits, and matrix $ A $ together with the vector $ b $ gives the set of technological constraints. . An object orientated approach will be used to describe the space of the problem. . class production_portfolio: &#39;&#39;&#39;This class describes the model of optimal product portfolio choice It takes in constraints and prices with the ability to remove each&#39;&#39;&#39; prices = [] #The constraints are added as a class attribute, NOT an instance attribute #To delete constraints, must use the remove_constraint method individually or the reset method to remove all a_matrix = [] b_matrix = [] temp = [] def __init__(self, n): &#39;&#39;&#39;Initialize the optimal product portfolio choice model&#39;&#39;&#39; #n is the number of constraints (not including non-negativity constraints) #Took out current specification (label) of the model - not needed for our model self.n = n def __repr__(self): &#39;&#39;&#39;String representation for the optimal product portfolio choice model&#39;&#39;&#39; return (&#39;&#39;&#39;These are the constraints: nA:{} nb:{} nThese are the prices: {} nThere are {} constraints &#39;&#39;&#39;.format(self.a_matrix, self.b_matrix, self.prices, self.nr_constraints())) def set_prices(self,p): &#39;&#39;&#39;Set product prices: p is list of nparray of length n&#39;&#39;&#39; p = np.array(p) self.prices.append(list(abs(p))) return self.prices def add_constraint(self,coef,b): &#39;&#39;&#39; Adds the coefficients and b values to the model. Does not check if it is feasible. Run consistency_check below to check feasibility. &#39;&#39;&#39; self.a_matrix.append(coef) self.b_matrix.append(b) return self.a_matrix, self.b_matrix def inception(matrix): &#39;&#39;&#39;Converts nested lists to a one-nested list(i.e. a matrix form) [[[1,5], [4,6]]] -&gt; [[1,5], [4,6]]. Needs to be in this later form for below function&#39;&#39;&#39; if isinstance(matrix, list): temp.append(matrix) matrix = matrix[0] inception(matrix) if isinstance(matrix, int): return temp[-1] return temp[-2] def remove_constraint(self,j): &#39;&#39;&#39;Removes the jth row constraint of the model&#39;&#39;&#39; self.a_matrix = inception(self.a_matrix) self.a_matrix = np.delete(self.a_matrix, j, axis=0) self.b_matrix = np.delete(self.b_matrix, j, axis=0) #If prices not equal to the number of products, this sets the remaining equal to 0. Might not be needed if len(self.prices)&gt;0: self.prices = np.delete(self.prices,j) return self.a_matrix, self.b_matrix, self.prices return self.a_matrix, self.b_matrix def nr_constraints(self): &#39;&#39;&#39;Number of constraints in the current model&#39;&#39;&#39; return (np.array(self.a_matrix).shape[0]) @classmethod def reset(self): &#39;&#39;&#39;Removes all the constraints in the model from the class attribute&#39;&#39;&#39; self.a_matrix = [] self.b_matrix = [] self.prices = [] . A consistency check for the problem needs to be added. How do you tell if a solution exists within the constraints? . upperbound = 20 grid = np.linspace(0, upperbound, 50) hypercube = [] #This is the space of x #Each element in grid is multipled by all the other elements in grid to create our hypercube #Not a proper hypergrid as the dimensions are (2500,2) but is an unstack of each dimension onto a column for i in grid: for j in grid: hypercube.append([j,i]) hypercube = np.array(hypercube) x = hypercube . def consistency_check(a, b,x, verbose=False): &#39;&#39;&#39;Checks if the system of inequality constraints in the model is consistent&#39;&#39;&#39; # x is a 2500x2 matrix, a is the constraint matrix #Each element in x is checked against the equation a*x &lt;= b #The first constraint in which this equation is not true, it exits out of the loop and goes to the next x. #Repeats until it finds an x that meets all the constraints, then exits the loop with True else False for i in range(len(x)): for j in range(len(a)): if not a[j]@x[i] &lt;= b[j]: break if j == (len(a)-1): if verbose: return print(&quot;This is feasible at: ni: {}, a: {}, b: {}, j:{}&quot;.format(i, a[j], b[j], j)) else: return True if verbose: return print(&quot;This is not feasible&quot;) else: return False . class TestDemo(unittest.TestCase): def test1(self): &#39;&#39;&#39;general feasible constraints&#39;&#39;&#39; a = [[1,1], [3,2],[-1,0],[0,-1]] b = [2,5,0,0] test = consistency_check(a,b,x) self.assertTrue(test) def test2(self): &#39;&#39;&#39;general feasible constraints&#39;&#39;&#39; a = [[1,1], [1,0], [-1,0], [0,-1]] b = [1,0,0,0] test = consistency_check(a,b,x) self.assertTrue(test) def test3(self): &#39;&#39;&#39;non-feasible constraints with given x&#39;&#39;&#39; a = [[1,1], [3,2], [-1, 1], [-1,0], [0,-1]] b = [2,5,-5,0,0] test = consistency_check(a,b,x) self.assertFalse(test) #This is not feasible and should return False def test4(self): &#39;&#39;&#39;non-feasible constraints with given x&#39;&#39;&#39; a = [[-0.5, -1],[1,1],[-1,0],[0,-1]] b = [-12,10,0,0] test = consistency_check(a,b,x) self.assertFalse(test) #This is not feasible and should return False if __name__ == &#39;__main__&#39;: unittest.main(argv=[&#39;first-arg-is-ignored&#39;], exit=False) . .... - Ran 4 tests in 0.061s OK . A Graphical Representation of the linear programming problem . class graph_from_matrix: def __init__(self, matrix, x, n, contour=None, nonneg=True, xlim=10,ylim=10, verbose=False, solve=False, geq=None, c = None, overlap = False, optimal_level = False, hide_point=False): self.matrix = matrix #Matrix of constraints. Must include non-negativity constraints in here. self.x = x #Space of solutions self.n = n # Number of constraints to solve self.nonneg = nonneg #Automatically fills in nonnegativity constraints if true self.xlim = xlim #Sets limits on x self.ylim = ylim #Sets limits on y self.verbose=verbose self.solve = solve #If constraints or objective functions are given, can calculate the optimal point self.geq = geq #The shaded region between the constraint and axis is flipped for a particular constraint self.contour = contour # True or False self.c = c #Vector of prices/objective fuction self.optimal_level = optimal_level #Given a specific objective function, shows the specific contour plot self.overlap = overlap #Visualization tool to view price contour plot if it overlaps constraint self.hide_point = hide_point def equation(self, matrix): &#39;&#39;&#39;Takes in a matrix of A and b values, where the last column is the solution (b vector) and finds and equation to solve&#39;&#39;&#39; # Treats x and y as symbols and not variables x_symb = symbols(&#39;x&#39;); y_symb=symbols(&#39;y&#39;) expr = [] #Converts the matrices back into equations to solve for eq in range(len(self.matrix)): expr.append(self.matrix[eq][0]*x_symb + self.matrix[eq][1]*y_symb - self.matrix[eq][2]) lines = [] # Solves the equations for y for each equation found above for equation in expr: lines.append(solve(equation, y_symb)) if self.verbose: print(&quot;These are the equations to solve: {}&quot;.format(lines[:-2])) # Takes off the last two values which are the non-negativity constraints. They are 0 return lines[:-2] def yvalue(self): &#39;&#39;&#39;Solves for y given the above equation and the x&#39;s formed from the meshed grid&#39;&#39;&#39; #Calls above function with the given instanced matrix equations = self.equation(self.matrix) x = self.x #Needed to evaluate the type str below y = [] #Evaluates y for each equation for i in equations: y.append(eval(str(i[0]))) # Dimensions equal to amount of constraints solved. Doesn&#39;t solve non-negativity constraints return y def plot(self): &#39;&#39;&#39;Plots the equations, the mesh grid, contour plots and optimal point/s&#39;&#39;&#39; fig, ax1 = plt.subplots() plt.rcParams[&#39;figure.figsize&#39;] = [12, 8] #Only used if the constraint overlaps a specific contour plot given by a price vector #Plots the overlapped constraint with a small linewidth and the rest as normal if self.overlap: for i in range(len(self.yvalue())): if i == 2: plt.plot(self.x, self.yvalue()[i], color=&#39;black&#39;, linewidth=0.01) else: plt.plot(self.x, self.yvalue()[i], color=&#39;black&#39;) else: for i in self.yvalue(): plt.plot(self.x, i, color=&#39;black&#39;) #Fills in each constraint as necessary, geq flips the fill to the opposite side #Colours in the feasible set given the constraints for i in range(len(self.yvalue())): if i == self.geq: ax1.fill_between(self.x,max(self.yvalue()[i]),self.yvalue()[i],alpha=0.33,color=&#39;g&#39;) else: ax1.fill_between(self.x,0,self.yvalue()[i],alpha=0.33,color=&#39;g&#39;) #Plots the mesh grid, each dimension of a column is a dimension of a hypergrid. In this case = square plt.scatter(x[:,0], x[:,1], s=3) # Plots lines on axis if non-negativity constraint is needed if self.nonneg: plt.plot([0]*self.ylim,range(0,self.ylim),color=&#39;black&#39;) plt.plot(range(0,self.xlim),[0]*self.xlim,color=&#39;black&#39;) #Maximum x and y values are equal to the maximum of the constraint matrices plt.xlim(0,(self.matrix).max()) plt.ylim(0,(self.matrix).max()) plt.xlabel(&#39;x&#39;) plt.ylabel(&#39;y&#39;) #If the objective function and constraints exists, the equation is solved for the optimal point and graphed #If the value of the optimal solution does not exist in our space of values (meshgrid) #the nearest point that exists to our optimal solution is found and also graphed if self.solve: point = solver(n=2,c=c,A=A,b=b)[&#39;x&#39;] if not self.hide_point: plt.plot(point[0],point[1],&#39;ro&#39;) plt.plot(nearest(hypercube, point)[0], nearest(hypercube, point)[1], &#39;bo&#39;) if self.contour: X,Y = np.meshgrid(grid, grid) Z = self.contour[0]*X + self.contour[1]*Y #If a specific objective function is given, this graphs only that contour line if self.optimal_level: self.optimal_level = self.contour[0]*point[0]+self.contour[1]*point[1] ax1.contour(X,Y,Z, levels=[self.optimal_level],colors=&#39;r&#39;, linewidths=3) #If no specific objective function, maps out a variety of contours else: ax1.contour(X,Y,Z, levels=30,colors=&#39;r&#39;) return plt.show() . Example problem with a given a, b . x = hypercube a = np.array([[1,1], [3,2],[-1,0],[0,-1]]) #Constraints/coefficients b = np.array([2,5,0,0]).reshape(4,1) #Technological constraints matrix = np.append(a,b,1) n = 2 obj = graph_from_matrix(matrix, x[:,0], n, contour=[5,3]) obj.plot() . def intersection_points(n,*args): x_points = [] y_points = [] for i in range(len(args)): for j in range(len(args)): if i != j and j &gt;= i: #print(&quot;Solving for: {} = {}&quot;.format(args[i], args[j])) x_points.append(solve(args[i]-args[j],symbols(&#39;x&#39;))) for i in range(len(x_points)): x_points[i] = float(x_points[i][0]) if n == 3: temp = x_points.copy() x_points[2] = temp[1] x_points[1] = temp[2] #print(x_points, args, sep=&#39; n&#39;) for x_values, equation in zip(x_points,args): x = x_values y_points.append(eval(str(equation))) #return x_points return list(zip(x_points, y_points)) . intersection = intersection_points(2, obj.equation(matrix)[0][0], obj.equation(matrix)[1][0]) print(&quot;The intersection point of the above graph is x: {}, y: {}&quot;.format(intersection[0][0], intersection[0][1])) . The intersection point of the above graph is x: 1.0, y: 1.0 . Given a price vector, what is the optimal value? . Example: . Let $ n=2 $ and the goods production technology is restricted by . $$ begin{cases} y - x &amp; le&amp; 6, 2x - y &amp; le&amp;12, end{cases} $$In addition, there is a resource constraint given by $ x + 2.5y le 16 $. . Finally, let profit be given by $ pi(x,y) = 3y + 5x $. . def solver(n, c, A, b, method=&#39;simplex&#39;): &#39;&#39;&#39; Uses linprog to solve optimal production portfolio n is number of goods c = the profit function A is the matrix of constraints B is the technology constraint&#39;&#39;&#39; f = production_portfolio(n) for i in range(len(A)): f.add_constraint(A[i],b[i]) A = f.a_matrix b = f.b_matrix f.prices = c c = np.array(c) res = linprog(c, A_ub = A, b_ub = b, method=method) return res . #Constraints A = [[-1, 1],[2, -1],[1, 2.5],[-1, 0],[0, -1]] b = [6,12,16,0,0] #Prices c = [-5,-3] f = production_portfolio(3) point = solver(n=3,c=c,A=A,b=b)[&#39;x&#39;] f.prices = c f . These are the constraints: A:[[-1, 1], [2, -1], [1, 2.5], [-1, 0], [0, -1]] b:[6, 12, 16, 0, 0] These are the prices: [-5, -3] There are 5 constraints . print(&#39;This is the optimal solution given by the above constraints nx:{} ny:{}&#39;.format(round(point[0],3), round(point[1],3))) . This is the optimal solution given by the above constraints x:7.667 y:3.333 . If it&#39;s a discrete product portfolio, the solution might not exist within our space of values (the grid/hypercube). Need to find the feasible point closest to the optimal solution . def nearest(x, optimal_points, verbose=False): &#39;&#39;&#39;As the product space may not be continious, the closest value to the optimal solution is found in our grid&#39;&#39;&#39; #Takes the absolute difference between the optimal point and all the values of the mesh grid #Finds the index of the smallest difference index = ((np.abs(x[:,0] - optimal_points[0])) + (np.abs(x[:,1] - optimal_points[1]))).argmin() feasible = True #For that value, see if it is feasible. If not feasible, delete that value and redo fnding the nearest point while feasible: # If this evaluates to True, feasible = false. exits loop feasible = not (A@np.array(x[index]) &lt;= b.T).all() if not feasible: break if verbose: print(&#39;Deleted these: {}&#39;.format(x[index])) x = np.delete(x, index, 0) index = ((np.abs(x[:,0] - optimal_points[0])) + (np.abs(x[:,1] - optimal_points[1]))).argmin() return x[index] . x = hypercube #New constraints a = np.array([[-1, 1],[2, -1],[1, 2.5],[-1, 0],[0, -1]]) b = np.array([6,12,16,0,0]).reshape(5,1) #Added together into a matrix matrix = np.append(a,b,1) #Price vector is negative as it is a maximization problem and by default linprog solves for minimization c = [-5,-3] contour = list(np.array(c)*-1) n = 3 #Matrix = constriants, x=[:,0] hypercube, n = 3, find the optimal point obj = graph_from_matrix(matrix, x[:,0], n, solve=True, geq=1, contour=contour, c=c, optimal_level=True) obj.plot() . intersection = intersection_points(3, obj.equation(matrix)[0][0], obj.equation(matrix)[1][0], obj.equation(matrix)[2][0]) print(&#39;&#39;&#39;The intersection points of the above graph are the following: nx1: {} y1: {} nx2: {} y2: {} nx3&quot; {} y3: {}&#39;&#39;&#39;.format(intersection[0][0], intersection[0][1], intersection[1][0], intersection[1][1], intersection[2][0], intersection[2][1])) . The intersection points of the above graph are the following: x1: 18.0 y1: 24.0 x2: 7.666666666666667 y2: 3.333333333333334 x3&#34; 0.2857142857142857 y3: 6.2857142857142865 . Short Analysis on the given price ratio . Example 1: Price Ratio at 2x:5y . Due to prices always being positive, the optimal bundle is monotonic and always limited by the constraints. If the price vector is parallel to the constraint $ x + 2.5y le 16 $, the optimal bundle will be any point along this constraint between the two extreme points of (7.667, 3.333) and (0.286, 6.286) . x = hypercube a = np.array([[-1, 1],[2, -1],[1, 2.5],[-1, 0],[0, -1]]) b = np.array([6,12,16,0,0]).reshape(5,1) matrix = np.append(a,b,1) contour = [3,7.5] c = [-3,-7.5] n = 3 obj = graph_from_matrix(matrix, x[:,0], n, solve=True, geq=1, contour=contour, c=c, optimal_level=True, overlap=True, hide_point=True) obj.plot() . Example 2: Ratio lower than 2:5 . In this example, we pick an objective function where the ratio of price x to price y is marginally smaller than 2:5. We can see that the flatter objective function means (0.286, 6.286) becomes the optimal point. . x = hypercube a = np.array([[-1, 1],[2, -1],[1, 2.5],[-1, 0],[0, -1]]) b = np.array([6,12,16,0,0]).reshape(5,1) matrix = np.append(a,b,1) contour = [5,13] c = [-5,-13] n = 3 obj = graph_from_matrix(matrix, x[:,0], n, solve=True, geq=1, optimal_level=True, contour=contour, c=c, overlap=True) obj.plot() .",
            "url": "https://ianldias.github.io/Portfolio/2020/11/14/LinearProgramming.html",
            "relUrl": "/2020/11/14/LinearProgramming.html",
            "date": " • Nov 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ianldias.github.io/Portfolio/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Resume",
          "content": ".",
          "url": "https://ianldias.github.io/Portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ianldias.github.io/Portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}